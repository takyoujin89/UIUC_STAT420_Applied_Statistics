---
title: 'Linear Models: Three Simulation Studies'
author: "Summer 2018, Unger"
date: "Example Solution"
output:
  html_document:
    theme: flatly
    toc: yes
    fig_width: 10
    fig_height: 5
  pdf_document:
    toc: yes
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
library(knitr)
opts_chunk$set(cache = TRUE, autodep = TRUE)
```

# Overview

In this document we will perform three simulation studies related to linear regression.

- **Study 1** will look at the significance of regression test in multiple regression.
- **Study 2** will evaluate Test RMSE as a metric for selecting a model.
- **Study 3** will investigate the power of the significance of regression test for SLR.

# Study 1: Significance of Regression

In this simulation study we will investigate the significance of regression test. We will simulate from two different models:

1. The **"significant"** model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 3$,
- $\beta_1 = 1$,
- $\beta_2 = 1$,
- $\beta_3 = 1$.

2. The **"non-significant"** model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 3$,
- $\beta_1 = 0$,
- $\beta_2 = 0$,
- $\beta_3 = 0$.

For both, we will consider a sample size of $25$ and three possible levels of noise. That is, three values of $\sigma$.

- $n = 25$
- $\sigma \in (1, 5, 10)$

We use simulation to obtain an empirical distribution for each of the following values, for each of the three values of $\sigma$, for both models.

- The **$F$ statistic** for the significance of regression test.
- The **p-value** for the significance of regression test
- **$R^2$**

We use the data found in [`study_1.csv`](study_1.csv) for the values of the predictors which are kept constant for the entirety of this study. The `y` values in this data are a blank placeholder.

## Methods

Here we show the most relevant `R` code for obtaining the simulated values. See the `.Rmd` files for additional `R` code not seen in this document. Specifically, we simulate a sample of size 25 a total of 2500 times for each value of $\sigma$. Each time, after generating the new $y$ data, we fit a model, and store the three values of interest. We also perform some calculations that will be useful for displaying and discussing the results.

```{r, echo = FALSE}
# clear enviroment
rm(list = ls())

# some  parameters
sample_size = 25

# data generation for project
gen_sim_data = function(seed = 42, n = 400) {

  set.seed(seed)

  data.frame(
    y  = rep(x = 0, times = n),
    x1 = round(runif(n = n, min = -5, max = 1), 2),
    x2 = round(runif(n = n, min =  2, max = 3), 1),
    x3 = runif(n = n, min =  2, max = 3)
  )

}

# generate data
sim_data = gen_sim_data(seed = 1337, n = sample_size)
write.csv(sim_data, "study_1.csv", row.names = FALSE)
```

```{r, echo = FALSE}
# set seed
birthday = 18760613
set.seed(birthday)
```

```{r}
# read in predictor data
sim_data = read.csv("study_1.csv")

# load libraries
library(broom)

# some simulation parameters
sample_size = 25
num_sims    = 2500

# function for performing each individual simulation
perform_sim = function(beta = 0, sigma = 5) {

  # simulate y data
  sim_data$y = with(sim_data, 3 + beta * x1 + beta * x2 + beta * x3 +
                    rnorm(n = sample_size, mean = 0, sd = sigma))

  # fit model to simulated data
  fit = lm(y ~ x1 + x2 + x3, data = sim_data)

  # extract the three desired estimates
  c(f = glance(fit)$statistic,
    p_val = glance(fit)$p.value,
    R2 = glance(fit)$r.squared)
}

# get estimates for each sigma value, significant model
sigma_01_sig = t(replicate(n = num_sims, perform_sim(beta = 1, sigma = 1)))
sigma_05_sig = t(replicate(n = num_sims, perform_sim(beta = 1, sigma = 5)))
sigma_10_sig = t(replicate(n = num_sims, perform_sim(beta = 1, sigma = 10)))

# get estimates for each sigma value, non-significant model
sigma_01_non = t(replicate(n = num_sims, perform_sim(beta = 0, sigma = 1)))
sigma_05_non = t(replicate(n = num_sims, perform_sim(beta = 0, sigma = 5)))
sigma_10_non = t(replicate(n = num_sims, perform_sim(beta = 0, sigma = 10)))
```

## Results

### Significant Model

```{r, echo = FALSE}
black = "#2C3E50"
grey = "#ECF0F1"
green = "#009999"
```

```{r, echo = FALSE}
par(mfrow = c(1, 3), oma = c(0, 0, 5, 0))
hist(sigma_01_sig[, "f"], breaks = 25, border = black, prob = TRUE, 
     xlab = expression(F), main = expression(sigma ~ ' = 1'), ylim = c(0, 0.5),
     col = grey, cex.main = 2)
hist(sigma_05_sig[, "f"], breaks = 25, border = black, prob = TRUE, 
     xlab = expression(F), main = expression(sigma ~ ' = 5'), ylim = c(0, 0.5),
     col = grey, cex.main = 2)
hist(sigma_10_sig[, "f"], breaks = 25, border = black, prob = TRUE, 
     xlab = expression(F), main = expression(sigma ~ ' = 10'), ylim = c(0, 0.5),
     col = grey, cex.main = 2)
mtext(expression('Significant Model: Empirical Distributions of ' ~ F), outer = TRUE, cex = 1.5)
```

```{r, echo = FALSE}
par(mfrow = c(1, 3), oma = c(0, 0, 5, 0))
hist(sigma_01_sig[, "p_val"], breaks = 25, border = black, prob = TRUE,
     xlab = expression(p-value), main = expression(sigma ~ ' = 1'), 
     col = grey, cex.main = 2)
hist(sigma_05_sig[, "p_val"], breaks = 25, border = black, prob = TRUE,
     xlab = expression(p-value), main = expression(sigma ~ ' = 5'), 
     col = grey, cex.main = 2)
hist(sigma_10_sig[, "p_val"], breaks = 25, border = black, prob = TRUE,
     xlab = expression(p-value), main = expression(sigma ~ ' = 10'), 
     col = grey, cex.main = 2)
mtext(expression('Significant Model: Empirical Distributions of p-value'), outer = TRUE, cex = 1.5)
```

```{r, echo = FALSE}
par(mfrow = c(1, 3), oma = c(0, 0, 5, 0))
hist(sigma_01_sig[, "R2"], breaks = 25, border = black, prob = TRUE, 
     xlab = expression(R^2), main = expression(sigma ~ ' = 1'), xlim = c(0, 1),
     col = grey, cex.main = 2)
hist(sigma_05_sig[, "R2"], breaks = 25, border = black, prob = TRUE, 
     xlab = expression(R^2), main = expression(sigma ~ ' = 5'), xlim = c(0, 1),
     col = grey, cex.main = 2)
hist(sigma_10_sig[, "R2"], breaks = 25, border = black, prob = TRUE, 
     xlab = expression(R^2), main = expression(sigma ~ ' = 10'), xlim = c(0, 1),
     col = grey, cex.main = 2)
mtext(expression('Significant Model: Empirical Distributions of ' ~ R^2), outer = TRUE, cex = 1.5)
```

### Non-Significant Model

```{r, echo = FALSE}
par(mfrow = c(1, 3), oma = c(0, 0, 5, 0))
hist(sigma_01_non[, "f"], breaks = 25, border = black, prob = TRUE, 
     xlab = expression(F), main = expression(sigma ~ ' = 1'), ylim = c(0, 1),
     col = grey, cex.main = 2)
curve(df(x, df1 = 3 - 1, df2 = 25 - 3), col = green, add = TRUE, lwd = 2)
hist(sigma_05_non[, "f"], breaks = 25, border = black, prob = TRUE, 
     xlab = expression(F), main = expression(sigma ~ ' = 5'), ylim = c(0, 1),
     col = grey, cex.main = 2)
curve(df(x, df1 = 3 - 1, df2 = 25 - 3), col = green, add = TRUE, lwd = 2)
hist(sigma_10_non[, "f"], breaks = 25, border = black, prob = TRUE, 
     xlab = expression(F), main = expression(sigma ~ ' = 10'), ylim = c(0, 1),
     col = grey, cex.main = 2)
curve(df(x, df1 = 3 - 1, df2 = 25 - 3), col = green, add = TRUE, lwd = 2)
mtext(expression('Non-Significant Model: Empirical Distributions of ' ~ F), outer = TRUE, cex = 1.5)
```

```{r, echo = FALSE}
par(mfrow = c(1, 3), oma = c(0, 0, 5, 0))
hist(sigma_01_non[, "p_val"], breaks = 25, border = black, prob = TRUE,
     xlab = expression(p-value), main = expression(sigma ~ ' = 1'), 
     col = grey, cex.main = 2)
curve(dunif(x, min = 0, max = 1), col = green, add = TRUE, lwd = 2)
hist(sigma_05_non[, "p_val"], breaks = 25, border = black, prob = TRUE,
     xlab = expression(p-value), main = expression(sigma ~ ' = 5'), 
     col = grey, cex.main = 2)
curve(dunif(x, min = 0, max = 1), col = green, add = TRUE, lwd = 2)
hist(sigma_10_non[, "p_val"], breaks = 25, border = black, prob = TRUE,
     xlab = expression(p-value), main = expression(sigma ~ ' = 10'), 
     col = grey, cex.main = 2)
curve(dunif(x, min = 0, max = 1), col = green, add = TRUE, lwd = 2)
mtext(expression('Non-Significant Model: Empirical Distributions of p-value'), outer = TRUE, cex = 1.5)
```

```{r, echo = FALSE}
par(mfrow = c(1, 3), oma = c(0, 0, 5, 0))
hist(sigma_01_non[, "R2"], breaks = 25, border = black, prob = TRUE, 
     xlab = expression(R^2), main = expression(sigma ~ ' = 1'), xlim = c(0, 1),
     col = grey, cex.main = 2)
curve(dbeta(x, shape1 = 3 / 2, shape2 = (21) / 2), col = green, add = TRUE, lwd = 2)
hist(sigma_05_non[, "R2"], breaks = 25, border = black, prob = TRUE, 
     xlab = expression(R^2), main = expression(sigma ~ ' = 5'), xlim = c(0, 1),
     col = grey, cex.main = 2)
curve(dbeta(x, shape1 = 3 / 2, shape2 = (21) / 2), col = green, add = TRUE, lwd = 2)
hist(sigma_10_non[, "R2"], breaks = 25, border = black, prob = TRUE, 
     xlab = expression(R^2), main = expression(sigma ~ ' = 10'), xlim = c(0, 1),
     col = grey, cex.main = 2)
curve(dbeta(x, shape1 = 3 / 2, shape2 = (21) / 2), col = green, add = TRUE, lwd = 2)
mtext(expression('Non-Significant Model: Empirical Distributions of ' ~ R^2), outer = TRUE, cex = 1.5)
```

## Discussion

First, we note that in the case of the "non-significant" model, the null hypothesis of the significant of regression test is true, so we know the distributions of the requested estimates.

For $F$,

\[
F \sim F(p - 1, n - p).
\]

For the p-value,

\[
\text{p-value} \sim \text{Unif}(0, 1).
\]


For $R ^ 2$,

\[
R ^ 2 \sim \text{Beta}\left(\frac{p}{2}, \frac{n - p - 1}{2}\right).
\]

These results are mostly "uninteresting." From the course, we already knew the distribution of the $F$ statistic. Essentially we're verifying that the $F$ statistic has an $F$ distribution. While that itself isn't interesting, it is somewhat interesting to note that the distribution of $F$ in this non-significant case does not depend on the level of noise, $\sigma$.

While not directly discussed in the course, it is a well-known fact in hypothesis testing that the distribution of the p-value is uniform if the null hypothesis is true. This should make sense. Assuming the null is true, $\alpha$ controls how many tests will be rejected. So because the null is true here, for any value of $\sigma$, the proportion of tests rejected is roughly $\alpha$, for any $\alpha$!

[While it isn't particularity useful to know, it turns out the we could derive the distribution of $R^2$ in this situation.](https://stats.stackexchange.com/questions/130069/what-is-the-distribution-of-r2-in-linear-regression-under-the-null-hypothesis). More interestingly, in this non-significant case, the distribution of $R^2$ does not depend on $\sigma$.

For comparison with our empirical results, we have added the true distributions to the above histograms. Note that if we were to increase the number of simulations performed, the histograms would better match the true distributions.

Moving to the significant model, the most interesting discussion is comparison with the non-significant model. For the $F$ statistic, we immediately see that its distribution is dependent on $\sigma$. The larger the value of $\sigma$, the less varied the distribution.

More interesting is what happens to the p-value. Since we know that this model is truly significant, we would hope that the vast majority of these tests are rejected. We see that is the case when $\sigma = 1$. However, as we increase $\sigma$, the distribution is still heavily weighted towards very small p-values, but increasingly contains much larger p-values that would not reject. Essentially, the more noise, the harder it is to detect signal. More on this in Study 3!

Lastly, we see that $\sigma$ has a large effect on $R^2$. In particular we see that $R^2$ is often rather small when $\sigma$ is larger, even though the model is truly significant!

## Instructor Comments

- A general attempt is made throughout this document to show reasonable code (in Methods) and hide plotting code (in Results). Generally, for a report like this, the code to generate the simulations is informative. For plots, the graphics themselves are informative, while the code to generate them, not as much. (The somewhat sloppy code to produce these graphics can be found in the `.Rmd` file.)

- The discussion question were meant to hint at possible topics to discuss and were only suggestions. They were not necessarily meant to be answered directly.

- Be aware of the axes used in the plots. Sometimes they are held constant for similar plots, sometimes not, depending on what information we are try to convey.

# Study 2: RMSE for Selection?

In homework we saw how Test RMSE can be used to select the "best" model. In this simulation study we will investigate how well this procedure works. Since splitting the data is random, we don't expect it to work correctly each time. We could get unlucky. But averaged over many attempts, we should expect it to select the appropriate model.

We will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5} + \beta_6 x_{i6} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 0$,
- $\beta_1 = 5$,
- $\beta_2 = -4$,
- $\beta_3 = 1.6$,
- $\beta_4 = -1.1$,
- $\beta_5 = 0.7$,
- $\beta_6 = 0.3$.

We will consider a sample size of $500$ and three possible levels of noise. That is, three values of $\sigma$.

- $n = 500$
- $\sigma \in (1, 2, 4)$

We use the data found in [`study_2.csv`](study_2.csv) for the values of the predictors. These will be kept constant for the entirety of this study. The `y` values in this data are a blank placeholder.

Each time we simulate the data, we randomly split the data into train and test sets of equal sizes (250 observations for training, 250 observations for testing).

For each, fit **nine** models, with forms:

- `y ~ x1`
- `y ~ x1 + x2`
- `y ~ x1 + x2 + x3`
- `y ~ x1 + x2 + x3 + x4`
- `y ~ x1 + x2 + x3 + x4 + x5`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6`, the correct form of the model
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9`

For each model, calculate Train and Test RMSE.

\[
\text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}
\]

## Methods

```{r, echo = FALSE}
# clear enviroment
rm(list = ls())
```

```{r, echo = FALSE}
sample_size = 500

gen_sim_data = function(seed = 42, n = 400) {

  set.seed(seed)
  x = round(runif(n = n, min = -1, max = 2), 2)

  data.frame(
    y  = rep(x = 0, times = n),
    x1 = runif(n = n, min = -5, max = 1),
    x2 = x,
    x3 = x ^ 2,
    x4 = runif(n = n, min = 0, max = 1),
    x5 = runif(n = n, min = 0, max = 1),
    x6 = round(rnorm(n = n), 1),
    x7 = runif(n = n, min = 0, max = 1),
    x8 = rnorm(n = n),
    x9 = rnorm(n = n)
  )

}

# generate data
sim_data = gen_sim_data(seed = 1337, n = sample_size)
write.csv(sim_data, "study_2.csv", row.names = FALSE)
```

```{r}
# read in predictor data
sim_data = read.csv("study_2.csv")

# some simulation parameters
sample_size = nrow(sim_data)
num_sims    = 1000
num_models  = 9
```

Here we create a specific function for RMSE that will be useful for our model fitting strategy.

```{r}
rmse = function(model, data) {

  actual = data$y
  predicted = predict(model, data)
  sqrt(mean((actual - predicted) ^ 2))

}
```

```{r, echo = FALSE}
birthday = 18760613
set.seed(birthday)
```

We write a function for each simulation.

```{r}
perform_sim = function(sigma = 1) {

  # simulate y data
  sim_data$y = with(sim_data, 0 + 5 * x1 + -4 * x2 + 1.6 * x3 + -1.1 * x4 + 0.7 * x5 + 0.3 * x6 +
                      rnorm(n = sample_size, mean = 0, sd = sigma))

  # test-train split the data
  test_index = sample(x = sample_size, size = round(sample_size / 2))
  sim_test   = sim_data[test_index, ]
  sim_train  = sim_data[-test_index, ]

  model_list = list(
    fit1 = lm(y ~ x1, data = sim_train),
    fit2 = lm(y ~ x1 + x2, data = sim_train),
    fit3 = lm(y ~ x1 + x2 + x3, data = sim_train),
    fit4 = lm(y ~ x1 + x2 + x3 + x4, data = sim_train),
    fit5 = lm(y ~ x1 + x2 + x3 + x4 + x5, data = sim_train),
    fit6 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6, data = sim_train),
    fit7 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7, data = sim_train),
    fit8 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = sim_train),
    fit9 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, data = sim_train)
  )

  c(train_rsme = sapply(model_list, rmse, sim_train),
    test_rsme  = sapply(model_list, rmse, sim_test))

}
```

We then run the simulations for the varying noise levels.

```{r}
rmses_01 = replicate(n = num_sims, perform_sim(sigma = 1))
rmses_02 = replicate(n = num_sims, perform_sim(sigma = 2))
rmses_04 = replicate(n = num_sims, perform_sim(sigma = 4))
```

## Results

```{r, echo = FALSE}
ave_rmses_01 = apply(rmses_01, 1, mean)
ave_rmses_02 = apply(rmses_02, 1, mean)
ave_rmses_04 = apply(rmses_04, 1, mean)

results = data.frame(
  model_size        = 1:9,
  ave_train_rmse_01 = ave_rmses_01[1:num_models],
  ave_test_rmse_01  = ave_rmses_01[(num_models + 1):(num_models * 2)],
  ave_train_rmse_02 = ave_rmses_02[1:num_models],
  ave_test_rmse_02  = ave_rmses_02[(num_models + 1):(num_models * 2)],
  ave_train_rmse_04 = ave_rmses_04[1:num_models],
  ave_test_rmse_04  = ave_rmses_04[(num_models + 1):(num_models * 2)]
)
rownames(results) = NULL
```

```{r, echo = FALSE}
black = "#2C3E50"
grey = "#ECF0F1"
green = "#009999"
purple = "#990073"

par(mfrow = c(1, 3), oma = c(0, 0, 5, 0))
with(results, {
  plot(model_size, ave_train_rmse_01, type = "b",
       ylim = c(min(ave_train_rmse_01), max(ave_test_rmse_01)), col = green, lwd = 2,
       xlab = "Model Size", ylab = "Average RMSE", main = expression(sigma ~ ' = 1'))
  lines(model_size, ave_test_rmse_01, col = purple, type = "b", lwd = 2)
  legend("topright", c("Train", "Test"), col = c(green, purple), lty = 1, lwd = 2)

})
with(results, {
  plot(model_size, ave_train_rmse_02, type = "b",
       ylim = c(min(ave_train_rmse_02), max(ave_test_rmse_02)), col = green, lwd = 2,
       xlab = "Model Size", ylab = "Average RMSE", main = expression(sigma ~ ' = 2'))
  lines(model_size, ave_test_rmse_02, col = purple, type = "b", lwd = 2)
  legend("topright", c("Train", "Test"), col = c(green, purple), lty = 1, lwd = 2)
})
with(results, {
  plot(model_size, ave_train_rmse_04, type = "b",
       ylim = c(min(ave_train_rmse_04), max(ave_test_rmse_04)), col = green, lwd = 2,
       xlab = "Model Size", ylab = "Average RMSE", main = expression(sigma ~ ' = 4'))
  lines(model_size, ave_test_rmse_04, col = purple, type = "b", lwd = 2)
  legend("topright", c("Train", "Test"), col = c(green, purple), lty = 1, lwd = 2)
})
mtext(expression('Train versus Test RMSE'), outer = TRUE, cex = 1.5)
```

```{r, echo = FALSE}
par(mfrow = c(1, 3), oma = c(0, 0, 5, 0))
barplot(table(factor(apply(rmses_01[(num_models + 1):(num_models * 2),], 2, which.min), levels = 1:9)), xlab = "Model Size", ylab = "Selected", main = expression(sigma ~ ' = 1'), col = grey)
barplot(table(factor(apply(rmses_02[(num_models + 1):(num_models * 2),], 2, which.min), levels = 1:9)), xlab = "Model Size", ylab = "Selected", main = expression(sigma ~ ' = 2'), col = green)
barplot(table(factor(apply(rmses_04[(num_models + 1):(num_models * 2),], 2, which.min), levels = 1:9)), xlab = "Model Size", ylab = "Selected", main = expression(sigma ~ ' = 4'), col = purple)
mtext(expression('Distributions of Selected Model Size'), outer = TRUE, cex = 1.5)
```

## Discussion

Immediately, we notice that this procedure does not always select the correct model. Based on the Train vs Test plots, we do see that, **on average**, the procedure correctly selects the true model of size six. We also notice that, as the noise increases, the difference between train and test error increases.

```{r}
which.min(results$ave_test_rmse_01)
which.min(results$ave_test_rmse_02)
which.min(results$ave_test_rmse_04)
```

In the low noise case, $\sigma = 1$, we can see from the barplot, that most often the correct model is selected, but still not always. Also, the vast majority of cases contain all the the correct variables, but with some additional unneeded variables. Only very infrequently does the procedure select a model which is missing any of the truly significant variables.

With $\sigma = 2$, performance is similar, but now, more often there are truly significant variables left out of the chosen model.

For $\sigma = 4$, no longer is the correct model selected most often. Also again the number of truly significant variables left out of the chosen model increases. This noise level is too high for the given signal to be detected. (Magnitude of the $\beta$ parameters.)

## Instructor Comments

- In the coming weeks, we will look at more formalized methods of variable selection.
- Cross-validation will reduce the variability of this procedure.
- Note that here we are restricting ourselves to a certain set of well chosen model options. This was for convenience in this project, but we'll drop this restriction soon.
    - We only considered nested models. Because of this, selecting the correct number of predictors is equivalent to selecting the correct model. In general you could select the correct number, but choose the wrong predictors. When this is the case, we also need to be aware of false positives and false negatives. We'll investigate this when we return to more formalized methods of variable selection.
    - Additionally, the strength of signal decreased for each additional predictor.

# Simulation Study 3, Power

In this simulation study we will investigate the **power** of the significance of regression test for simple linear regression. 

\[
H_0: \beta_{1} = 0 \ \text{vs} \ H_1: \beta_{1} \neq 0
\]

Recall, we had defined the *significance* level, $\alpha$, to be the probability of a Type I error.

\[
\alpha = P[\text{Reject } H_0 \mid H_0 \text{ True}] = P[\text{Type I Error}]
\]

Similarly, the probability of a Type II error is often denoted using $\beta$; however, this should not be confused with a regression parameter.

\[
\beta = P[\text{Fail to Reject } H_0 \mid H_1 \text{ True}] = P[\text{Type II Error}]
\]

*Power* is the probability of rejecting the null hypothesis when the null is not true, that is, the alternative is true and $\beta_{1}$ is non-zero.

\[
\text{Power} = 1 - \beta = P[\text{Reject } H_0 \mid H_1 \text{ True}]
\]

Essentially, power is the probability that a signal of a particular strength will be detected. Many things affect the power of a test. In this case, some of those are:

- Sample Size, $n$
- Signal Strength, $\beta_1$
- Noise Level, $\sigma$
- Significance Level, $\alpha$

We'll investigate the first three.

To do so we will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$.

For simplicity, we will let $\beta_0 = 0$, thus $\beta_1$ is essentially controlling the amount of "signal." We will then consider different signals, noises, and sample sizes:

- $\beta_1 \in (-2, -1.9, -1.8, \ldots, -0.1, 0, 0.1, 0.2, 0.3, \ldots 1.9, 2)$
- $\sigma \in (1, 2, 4)$
- $n \in (10, 20, 30)$

We will hold the significance level constant at $\alpha = 0.05$.

## Methods

```{r, echo = FALSE}
# clear enviroment
rm(list = ls())
```

In an effort to keep our analysis "tidy," we utilize the `ggplot2` and `tidyr` packages.

```{r}
library(ggplot2)
library(tidyr)
```

We first write a function that will perform the individual simulations.

```{r}
get_p_val = function(n, beta_0, beta_1, sigma = 1) {
  x = seq(0, 5, length = n)
  y = beta_0 + beta_1 * x + rnorm(n, mean = 0, sd = sigma)
  fit = lm(y ~ x)
  summary(fit)$coefficients[2, 4]
}
```

```{r, echo = FALSE}
birthday = 18760613
set.seed(birthday)
```

We then setup a data frame to store the results, and run all of the simulations.

```{r}
results = expand.grid(
  beta = seq(-2, 2, by = 0.1),
  sigma = c(1, 2, 4),
  n = c(10, 20, 30),
  power = 0
)


for (i in 1:nrow(results)) {
  p_vals = replicate(n = 2500,
                     get_p_val(n = results$n[i],
                               beta_0 = 0,
                               beta_1 = results$beta[i],
                               sigma = results$sigma[i]))
  results$power[i] = mean(p_vals < 0.05)
}
```

## Results

We first plot power as a function of the signal strength for each noise level. The three curves in each plot represent a different sample size.

```{r, echo = FALSE}
results$n = as.factor(results$n)
ggplot(results, aes(x = beta, y = power, group = n, color = n)) + geom_line() + geom_point() + facet_grid(. ~ sigma)
```

We then plot power as a function of the signal to noise ratio, again for each noise level. The three curves in each plot represent a different sample size.

```{r, echo = FALSE}
ggplot(results, aes(x = beta / sigma, y = power, group = n, color = n)) + geom_line() + geom_point() + facet_grid(. ~ sigma)
```

Lastly, we plot power as a function of the signal strength for each sample size. The three curves in each plot represent a different noise level.

```{r, echo = FALSE}
results$sigma = as.factor(results$sigma)
ggplot(results, aes(x = beta, y = power, group = sigma, color = sigma)) + geom_line() + geom_point() + facet_grid(. ~ n)
```

## Discussion

The results are somewhat unsurprising.

- $\beta_1$: As $\beta_1$ increases, power increases. The stronger the signal, the easier it is to detect.
- $\sigma$: As $\sigma$ increases, power decreases. The more noise, the harder it is to detect signal.
- $n$: As $n$ increases, the power increases. A larger sample size makes it easier to detect a signal.

We actually see in the second series of plots that the signal to noise ratio is the actual driver of power.

When $\beta_1 = 0$, which we did simulate, the result is not actually power, but significance, and the results indeed match the specified significance levels.

Note that 2500 simulations were used to make slightly smoother curves. With only 1000 simulations the concepts are highlighted but the "curves" are somewhat jagged. Recall that we're not calculating the true power, we are estimating it with simulation. So the more simulations, the better the estimation.

## Instructor Comments

- `ggplot2` and `tidyr` are part of the larger [`tidyverse`](http://tidyverse.org/) which encompasses a number of useful packages.
